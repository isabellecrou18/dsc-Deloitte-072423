{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Analysis\" data-toc-modified-id=\"Principal-Component-Analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Principal Component Analysis</a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Motivation</a></span></li><li><span><a href=\"#Scenario:-Shipping-Costs\" data-toc-modified-id=\"Scenario:-Shipping-Costs-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Scenario: Shipping Costs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dimensionality\" data-toc-modified-id=\"Dimensionality-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Dimensionality</a></span></li><li><span><a href=\"#Correlation-and-Covariance-Matrices\" data-toc-modified-id=\"Correlation-and-Covariance-Matrices-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Correlation and Covariance Matrices</a></span></li><li><span><a href=\"#Eigendecomposition\" data-toc-modified-id=\"Eigendecomposition-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Eigendecomposition</a></span></li><li><span><a href=\"#Eigenvalues\" data-toc-modified-id=\"Eigenvalues-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Eigenvalues</a></span><ul class=\"toc-item\"><li><span><a href=\"#Proportion-of-Variance\" data-toc-modified-id=\"Proportion-of-Variance-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Proportion of Variance</a></span></li></ul></li><li><span><a href=\"#Eigenvectors-(aka-Principal-Components)\" data-toc-modified-id=\"Eigenvectors-(aka-Principal-Components)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Eigenvectors (aka Principal Components)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Orthogonality\" data-toc-modified-id=\"Orthogonality-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Orthogonality</a></span></li><li><span><a href=\"#First-Principal-Component\" data-toc-modified-id=\"First-Principal-Component-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>First Principal Component</a></span></li><li><span><a href=\"#Second-Principal-Component\" data-toc-modified-id=\"Second-Principal-Component-4.5.3\"><span class=\"toc-item-num\">4.5.3&nbsp;&nbsp;</span>Second Principal Component</a></span></li><li><span><a href=\"#Remaining-Principal-Components\" data-toc-modified-id=\"Remaining-Principal-Components-4.5.4\"><span class=\"toc-item-num\">4.5.4&nbsp;&nbsp;</span>Remaining Principal Components</a></span></li></ul></li><li><span><a href=\"#Sidebar:-Properties-of-Eigenvectors\" data-toc-modified-id=\"Sidebar:-Properties-of-Eigenvectors-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Sidebar: Properties of Eigenvectors</a></span></li><li><span><a href=\"#Transforming-Data\" data-toc-modified-id=\"Transforming-Data-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Transforming Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-Component\" data-toc-modified-id=\"First-Component-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>First Component</a></span></li><li><span><a href=\"#All-Components\" data-toc-modified-id=\"All-Components-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>All Components</a></span></li></ul></li><li><span><a href=\"#Feature-Correlations\" data-toc-modified-id=\"Feature-Correlations-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Feature Correlations</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#PCA-in-sklearn\" data-toc-modified-id=\"PCA-in-sklearn-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>PCA in <code>sklearn</code></a></span></li></ul></li><li><span><a href=\"#Scenario:-Car-Properties\" data-toc-modified-id=\"Scenario:-Car-Properties-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Scenario: Car Properties</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Prep\" data-toc-modified-id=\"Data-Prep-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Data Prep</a></span></li><li><span><a href=\"#Your-Model\" data-toc-modified-id=\"Your-Model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Your Model</a></span></li><li><span><a href=\"#Solution-Code\" data-toc-modified-id=\"Solution-Code-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Solution Code</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modeling-with-New-Dimensions\" data-toc-modified-id=\"Modeling-with-New-Dimensions-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Modeling with New Dimensions</a></span></li></ul></li><li><span><a href=\"#Visualizations\" data-toc-modified-id=\"Visualizations-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Visualizations</a></span></li><li><span><a href=\"#Level-Up:-Relation-to-Linear-Regression\" data-toc-modified-id=\"Level-Up:-Relation-to-Linear-Regression-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Level Up: Relation to Linear Regression</a></span></li><li><span><a href=\"#Level-Up:-Diagonalization\" data-toc-modified-id=\"Level-Up:-Diagonalization-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Level Up: Diagonalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Eigenvalues\" data-toc-modified-id=\"Eigenvalues-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Eigenvalues</a></span></li><li><span><a href=\"#Eigenvectors\" data-toc-modified-id=\"Eigenvectors-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Eigenvectors</a></span></li><li><span><a href=\"#Level-Up:-Diagonalization-In-Code\" data-toc-modified-id=\"Level-Up:-Diagonalization-In-Code-5.6.3\"><span class=\"toc-item-num\">5.6.3&nbsp;&nbsp;</span>Level Up: Diagonalization In Code</a></span></li></ul></li><li><span><a href=\"#Extra-Resource\" data-toc-modified-id=\"Extra-Resource-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Extra Resource</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irousseau\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.18.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.random.bit_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-adec5be98bdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNonBLASDotWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaskedArray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_MaskedArray\u001b[0m  \u001b[1;31m# TODO: remove in 0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    483\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_mstats_basic\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_continuous_distns\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\_discrete_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                     _check_shape, _ShapeInfo)\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_boost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m from ._biasedurn import (_PyFishersNCHypergeometric,\n\u001b[0m\u001b[0;32m     21\u001b[0m                         \u001b[0m_PyWalleniusNCHypergeometric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                         _PyStochasticLib3)\n",
      "\u001b[1;32mscipy\\stats\\_biasedurn.pyx\u001b[0m in \u001b[0;36minit scipy.stats._biasedurn\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.random.bit_generator'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Explain the concepts behind principal component analysis (PCA)\n",
    "- Explain how PCA addresses the problem of multicollinearity\n",
    "- Explain the idea of eigendecomposition\n",
    "- Implement PCA using `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So far, you have dealt with datasets with a moderate number of predictors. What would happen if you had thousands of predictors? A few problems can arise:\n",
    "\n",
    "* Requires a ton of computing power/time\n",
    "* Computational problems caused by multicollinearity\n",
    "* Can overfit your data\n",
    "\n",
    "How could we address this problem?\n",
    "\n",
    "* You could drop a bunch of predictors at random, but you would potentially lose useful information that way \n",
    "* You could drop predictors that have weak correlations with your target, but they may still be useful in combination with other features in non-linear models (e.g. interaction terms, decision trees) \n",
    "* You could combine a bunch of features together, such as by multiplying them, but it's not clear how you would do this to best preseve information\n",
    "\n",
    "Principal Component Analysis (PCA) is a tool for reducing the dimensionality of our data in a way that tries to preserve information. It does this by projecting our data from a higher-dimensional space onto a lower-dimensional space. The PCA algorithm chooses a lower-dimensional space to project to that will preserve as much variance as possible from our original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scenario: Shipping Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say that we want to predict the cost to ship a package based on its properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shipping Cost ($)</th>\n",
       "      <th>Length (in)</th>\n",
       "      <th>Width (in)</th>\n",
       "      <th>Height (in)</th>\n",
       "      <th>Weight (lb)</th>\n",
       "      <th>Distance (mi)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>1079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>1649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>1310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Shipping Cost ($)  Length (in)  Width (in)  Height (in)  Weight (lb)  \\\n",
       "0                 24           36          32            9           18   \n",
       "1                 38           41          31           12           18   \n",
       "2                 38           34          34           15           24   \n",
       "3                 20           32          22            8            8   \n",
       "4                 24           37          24           14           19   \n",
       "5                 21           32          20            3            6   \n",
       "6                 27           33          23            7           11   \n",
       "7                 31           40          31           16           22   \n",
       "8                 30           37          28           12           20   \n",
       "9                 26           29          29           11           11   \n",
       "\n",
       "   Distance (mi)  \n",
       "0           1079  \n",
       "1           1797  \n",
       "2           1722  \n",
       "3            461  \n",
       "4            491  \n",
       "5           1029  \n",
       "6            945  \n",
       "7           1649  \n",
       "8           1310  \n",
       "9            161  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packages = pd.read_csv('data/packages.csv')\n",
    "packages.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can think about each variable as a dimension, and thus each package as a data point. If we take just one feature, we can easily visualize this in 2 dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "packages.plot(kind='scatter', y='Shipping Cost ($)', x='Length (in)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can think of each package as a point in six-dimensional space - 5 dimensions for our features and 1 for our target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Correlation and Covariance Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first four features in this dataset all relate to package size, so we might expect them to be strongly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3d9aa711c450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m sns.heatmap(packages.corr(),\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.2g'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.heatmap(packages.corr(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PCA does not use this **correlation matrix**, which is conveniently scaled between -1 and 1. Rather, it uses the **covariance matrix**, which is scaled in square units of the original variables. This makes PCA very sensitive to the scale of the variables. Recall the covariance is given by:\n",
    "\n",
    "$$\\Large cov(x,y) = \\frac{\\sum_{i=1}^{N} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{N-1}$$\n",
    "\n",
    "and that the (pearsons) correlation is given by:\n",
    "\n",
    "$$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\bar{x})^2\\Sigma^n_{i = 1}(y_i -\\bar{y})^2}}$$\n",
    "\n",
    "so we can standardize the covariance by the standard deviations of X and Y (the $n$'s cancel!) and this gives us the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f2e008e94aef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m sns.heatmap(packages.cov(),\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.2g'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             cmap='coolwarm');\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.heatmap(packages.cov(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            center=0,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Continuing the idea from above, let's standardize our variables to mean = 0 & SD = 1, which will make our covariance matrix equal the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6b725f8a422c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpackages_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpackages\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpackages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpackages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m sns.heatmap(packages_scaled.cov(),\n\u001b[0m\u001b[0;32m      3\u001b[0m             \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.2g'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "packages_scaled = (packages - packages.mean())/packages.std()\n",
    "sns.heatmap(packages_scaled.cov(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            center=0,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we want to do now is to explain that spread, one linear transformation (one **eigenvector**) at a time. For more see [this useful blog post](https://datascienceplus.com/understanding-the-covariance-matrix/).\n",
    "\n",
    "Let's try to reduce the dimensionality of our dataset. Since the features capturing size are strongly correlated, we might expect to be able to reduce our feature dimensions down to two without losing much information (i.e. variance in our features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use an **eigendecomposition** of the covariance matrix to create a new set of dimensions. We can then decide how many of these dimensions to keep based on how much variance is captured by each dimension.\n",
    "\n",
    "Here, we show you how to do this using the NumPy `.eig()` function, but it is possible to do PCA more easily in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_packages_scaled = packages_scaled['Shipping Cost ($)']\n",
    "X_packages_scaled = packages_scaled.drop('Shipping Cost ($)', axis=1)\n",
    "\n",
    "cov_mat = X_packages_scaled.cov().values\n",
    "eigvals, eigvecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.86284654,  0.74652451,  0.73831054,  0.02624986],\n",
       "       [ 0.86284654,  1.        ,  0.83794881,  0.74739527,  0.01769847],\n",
       "       [ 0.74652451,  0.83794881,  1.        ,  0.86181886, -0.03204252],\n",
       "       [ 0.73831054,  0.74739527,  0.86181886,  1.        , -0.02456763],\n",
       "       [ 0.02624986,  0.01769847, -0.03204252, -0.02456763,  1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "cov_matcomposition gives us two things: eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Eigenvalues** represent the relative amount of variance captured by each new dimension (each new dimension being a covariance between the features given). The average eigenvalue will be 1, so we look for values over 1 to identify dimensions that capture more variance than average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eigvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It looks like we have one great dimension capturing 3.4x more variance than average, one OK dimension capturing an average amount of variance, and three other dimensions that don't capture much variance. This is in line with what we were expecting! It means that we can just use the first two dimensions - and drop the last three - without losing much variance/information from our predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Proportion of Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can also divide your eigenvalues by the number of features and then interpret them as the _proportion of variance in the features_ captured by each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eigvals/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Eigenvectors (aka Principal Components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Eigenvectors** represent the new dimensions, which we call **principal components** when doing PCA. There is one eigenvector for each dimension, and they are all combined together into one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In PCA, the values in our eigenvectors are called **component weights**, and they tell us how much variance of each feature is captured by that dimension. These weights range from -1 to 1, but the relative sizes are what matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These eigenvectors are **orthogonal**, meaning their dot product is zero. Think of it like being at right angles, like the x and y axes of a graph, but in higher-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eigvec1 = eigvecs[:, 0]\n",
    "eigvec2 = eigvecs[:, 1]\n",
    "eigvec1.dot(eigvec2)\n",
    "\n",
    "# will be perpendicular and the dot product is 0 (vector multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### First Principal Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first column of `eigvecs` is our first eigenvector, corresponding to the eigenvalue of 3.4. Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eigvec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that the first four numbers are relatively large, while the fifth is near zero. This means that this first dimension is almost entirely capturing the shared variance in our four size features, as we hoped! It's also interesting to note that the weights for the four features are almost equal, so they are equally represented in this dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Second Principal Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's look at our second eigenvector and see what features it seems to be capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eigvec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like it is almost entirely capturing the distance dimension, which makes sense, since that is not related to the package size at all. It has an eigenvalue of 1, which is appropriate, since the eigenvector only captures one feature, which wasn't captured at all in the first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remaining Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since the remaining eigenvalues were all much less than 1, we can ignore the eigenvectors associated with them. We will not include components corresponding to them in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Transforming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now use these principal components to create new features. These features will be weighted sums (aka **linear combinations**) of existing features, using the component weights from the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### First Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now create a new feature using the first principal component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eigvec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our first feature will be calculated as follows:\n",
    "\n",
    "**PC1** = 0.492 * Length + 0.508 * Width + 0.508 * Height + 0.492 * Weight - 0.003 * Distance\n",
    "\n",
    "We use a dot product between the data and the eigenvector to do the arithmetic for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_array = X_packages_scaled.values\n",
    "pc1 = data_array.dot(eigvec1)\n",
    "X_packages_pca = pd.DataFrame(data=pc1, columns=['PC1'])\n",
    "X_packages_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### All Components\n",
    "\n",
    "You can calculate all the new features at once using a dot product with the `eigvecs` matrix, which has all the eigenvectors in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pcs = data_array.dot(eigvecs)\n",
    "X_packages_pca = pd.DataFrame(data=pcs, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "X_packages_pca.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Feature Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Because we used eigenvectors to construct our new features, we have completely solved any multicollinearity issues. This is because the eigenvectors define new, uncorrelated dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(X_packages_pca.corr(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap= 'coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's compare linear regression models with...\n",
    "\n",
    "* All five original features \n",
    "* All five new features\n",
    "* Only 2 best new features\n",
    "* Only 1 best new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_packages_scaled, y_packages_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1: All base features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm1 = OLS(exog=X_train, endog=y_train).fit()\n",
    "sm1.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = sm1.predict(X_test)\n",
    "mse(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2: All PCs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eigval, tr_eigvec = np.linalg.eig(X_train.cov())\n",
    "\n",
    "X_tr_pca = X_train.dot(tr_eigvec)\n",
    "\n",
    "sm2 = OLS(exog=X_tr_pca, endog=y_train).fit()\n",
    "sm2.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat2 = sm2.predict(X_test.dot(tr_eigvec))\n",
    "mse(y_test, y_test_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 3 (Exercise!): Only PCs 1 and 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm3 = OLS(exog=X_tr_pca.loc[:, :2], endog=y_train).fit()\n",
    "print(sm3.rsquared)\n",
    "y_test_hat3 = sm3.predict(X_test.dot(tr_eigvec).loc[:, :2])\n",
    "mse(y_test, y_test_hat3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<code>sm3 = OLS(exog=X_tr_pca.loc[:, :2], endog=y_train).fit()\n",
    "print(sm3.rsquared)\n",
    "y_test_hat3 = sm3.predict(X_test.dot(tr_eigvec).loc[:, :2])\n",
    "mse(y_test, y_test_hat3)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 4 (Exercise!): Only PC 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm4 = OLS(exog=X_tr_pca.loc[:, 0], endog=y_train).fit()\n",
    "print(sm4.rsquared)\n",
    "y_test_hat4 = sm4.predict(X_test.dot(tr_eigvec).loc[:, 0])\n",
    "mse(y_test, y_test_hat4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<code>sm4 = OLS(exog=X_tr_pca.loc[:, 0], endog=y_train).fit()\n",
    "print(sm4.rsquared)\n",
    "y_test_hat4 = sm4.predict(X_test.dot(tr_eigvec).loc[:, 0])\n",
    "mse(y_test, y_test_hat4)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## PCA in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As always, `sklearn` makes this all much easier, this time with the `PCA()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # Check out how `n_components` works\n",
    "\n",
    "X_packages_pca2 = pca.fit_transform(X_packages_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can get the eigenvalues and eigenvectors out, too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes the signs get flipped on the eigenvectors - don't worry about it. Think of \"up\" and \"down\" as both representing the same dimension, just in opposite directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then you can use your transformed data as you would in any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_packages_pca2, y_packages_scaled)\n",
    "lr_pca.score(X_packages_pca2, y_packages_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scenario: Car Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use PCA to reduce the dimensionality of features in the example below: Predict car mpg using car properties. We've done the data prep. Now you practice the modeling, including scoring on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars[' cubicinches'].replace(' ', np.nan, inplace=True)\n",
    "cars[' cubicinches'] = cars[' cubicinches'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars[' weightlbs'].replace(' ', np.nan, inplace=True)\n",
    "cars[' weightlbs'] = cars[' weightlbs'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cars.drop('mpg', axis=1),\n",
    "                                                    cars['mpg'],\n",
    "                                                   random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ct1 = ColumnTransformer(transformers=[\n",
    "    ('imputer', SimpleImputer(), [1, 3])],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ct2 = ColumnTransformer(transformers=[\n",
    "    ('scaler', StandardScaler(), [0, 1, 2, 3, 4, 5]),\n",
    "    ('ohe', OneHotEncoder(), [6])],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('ct1', ct1),\n",
    "    ('ct2', ct2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_tr_pp = pipe.transform(X_train)\n",
    "X_te_pp = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## First Model w/o PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Let's start with a linear regression\n",
    "\n",
    "lr = LinearRegression().fit(X_tr_pp, y_train)\n",
    "\n",
    "## Score on train\n",
    "\n",
    "lr.score(X_tr_pp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Score on test\n",
    "\n",
    "lr.score(X_te_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the coefficients of the best-fit hyperplane\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$2.177\\times in^3\\_sd - 4.645\\times lbs.\\_sd - 1.555\\times cyl\\_sd - 1.154\\times hp\\_sd -  0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars_pca = PCA(n_components=3) \n",
    "\n",
    "X_train_new = cars_pca.fit_transform(X_tr_pp)\n",
    "X_test_new = cars_pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars_pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results of our PCA are as follows:\n",
    "\n",
    "**PC1** = 0.465 * cubicinches_sd + 0.435 * weightlbs_sd + 0.449 * cylinders_sd + 0.454 * hp_sd - 0.349 * time-to-60_sd - 0.187 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.140 * US\n",
    "\n",
    "**PC2** = -0.099 * cubicinches_sd - 0.196 * weightlbs_sd - 0.131 * cylinders_sd + 0.006 * hp_sd - 0.125 * time-to-60_sd - 0.937 * year_sd + 0.129 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.141 * cubicinches_sd + 0.342 * weightlbs_sd + 0.187 * cylinders_sd - 0.144 * hp_sd + 0.851 * time-to-60_sd - 0.239 * year_sd + 0.043 * Europe - 0.132 * Japan + 0.089 * US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Modeling with New Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have optimized our features, we can build a new model with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_train_new, y_train)\n",
    "lr_pca.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test_new = cars_pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_pca.score(X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_pca.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$-2.967\\times PC1 - 1.162\\times PC2 -2.486\\times PC3$\n",
    "\n",
    "Of course, since the principal components are just linear combinations of our original predictors, we could re-express this hyperplane in terms of those original predictors!\n",
    "\n",
    "And if the PCA was worth anything, we should expect the new linear model to be *different from* the first!\n",
    "\n",
    "Recall that we had:\n",
    "\n",
    "**PC1** = 0.465 * cubicinches_sd + 0.435 * weightlbs_sd + 0.449 * cylinders_sd + 0.454 * hp_sd - 0.349 * time-to-60_sd - 0.187 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.140 * US\n",
    "\n",
    "**PC2** = -0.099 * cubicinches_sd - 0.196 * weightlbs_sd - 0.131 * cylinders_sd + 0.006 * hp_sd - 0.125 * time-to-60_sd - 0.937 * year_sd + 0.129 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.141 * cubicinches_sd + 0.342 * weightlbs_sd + 0.187 * cylinders_sd - 0.144 * hp_sd + 0.851 * time-to-60_sd - 0.239 * year_sd + 0.043 * Europe - 0.132 * Japan + 0.089 * US\n",
    "\n",
    "Therefore, our new PCA-made hyperplane can be expressed as:\n",
    "\n",
    "$-2.967\\times(0.465\\times in^3\\_sd + 0.435\\times lbs.\\_sd + 0.449\\times cyl\\_sd + 0.454\\times hp\\_sd - 0.349\\times time_{60}\\_sd - 0.187\\times yr\\_sd - 0.068\\times brand_{Europe} - 0.073\\times brand_{Japan} + 0.140\\times brand_{US})$ <br/> $- 1.162\\times(-0.099\\times in^3\\_sd - 0.196\\times lbs.\\_sd - 0.131\\times cyl\\_sd + 0.006\\times hp\\_sd - 0.125\\times time_{60}\\_sd - 0.937\\times yr\\_sd + 0.129\\times brand_{Europe} + 0.022\\times brand_{Japan} - 0.152\\times brand_{US})$ <br/> $- 2.486\\times(0.141\\times in^3\\_sd + 0.342\\times lbs.\\_sd + 0.187\\times cyl\\_sd -0.144\\times hp\\_sd + 0.851\\times time_{60}\\_sd - 0.239\\times yr\\_sd + 0.043\\times brand_{Europe} - 0.132\\times brand_{Japan} + 0.089\\times brand_{US})$\n",
    "\n",
    "Let's make these calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'cubicinches_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 0], 3)}')\n",
    "print(f'weightlbs_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 1], 3)}')\n",
    "print(f'cylinders_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 2], 3)}')\n",
    "print(f'horsepower_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 3], 3)}')\n",
    "print(f'timeto60_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 4], 3)}')\n",
    "print(f'year_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 5], 3)}')\n",
    "print(f'Europe coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 6], 3)}')\n",
    "print(f'Japan coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 7], 3)}')\n",
    "print(f'US coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 8], 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So our best-fit hyperplane using PCA is:\n",
    "\n",
    "$-1.616\\times in^3\\_sd - 1.913\\times lbs.\\_sd - 1.646\\times cyl\\_sd - 0.996\\times hp\\_sd - 0.933\\times time_{60}\\_sd + 2.237\\times yr\\_sd - 0.055\\times brand_{Europe} + 0.517\\times brand_{Japan} - 0.462\\times brand_{US}$\n",
    "\n",
    "Recall that our first linear regression model had:\n",
    "\n",
    "$2.177\\times in^3\\_sd - 4.645\\times lbs.\\_sd - 1.555\\times cyl\\_sd - 1.154\\times hp\\_sd -  0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$\n",
    "\n",
    "which is clearly a different hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reassembling the whole dataset for the sake of visualization\n",
    "X_transformed = np.vstack([X_train_new, X_test_new])\n",
    "y_new = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 0], y_new, 'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 1], y_new, 'g.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 2], y_new, 'k.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack([X_transformed, y_new[:, np.newaxis]]),\n",
    "                  columns=['PC1', 'PC2', 'PC3', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.relplot(data=df,\n",
    "            x='PC1',\n",
    "            y='PC2',\n",
    "           hue='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extra Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Here's](https://www.youtube.com/watch?v=_UVHneBUBW0) a helpful video introduction to PCA if you're itching for more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
